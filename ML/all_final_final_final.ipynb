{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa10b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download()\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019f77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.txt\",delimiter=';',names=['text','label'])\n",
    "df_val = pd.read_csv(\"val.txt\",delimiter=';',names=['text','label'])\n",
    "df = pd.concat([df_train,df_val])\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "# df['label'].unique()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3690a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv(\"custom_dataset_for_train.csv\")\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4a27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2453bcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.enco(df)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enco(df):\n",
    "    df.replace(to_replace=\"surprise\",value=1,inplace=True)\n",
    "    df.replace(to_replace=\"joy\",value=2,inplace=True)\n",
    "    df.replace(to_replace=\"love\",value=2,inplace=True)\n",
    "    df.replace(to_replace=\"sadness\",value=3,inplace=True)\n",
    "    df.replace(to_replace=\"anger\",value=3,inplace=True)\n",
    "    df.replace(to_replace=\"fear\",value=1,inplace=True)\n",
    "    \n",
    "enco(df.label)\n",
    "enco(df_train)\n",
    "# enco\n",
    "# df.to_csv(\"df_enco.csv\")\n",
    "# df_train.to_csv(\"df_enco_train.csv\")\n",
    "# 1-Neutral\n",
    "# 2-Positive\n",
    "# 3-Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f6e5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>17995</td>\n",
       "      <td>im having ssa examination tomorrow in the morn...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>17996</td>\n",
       "      <td>i constantly worry about their fight against n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>17997</td>\n",
       "      <td>i feel its important to share this info for th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>17998</td>\n",
       "      <td>i truly feel that if you are passionate enough...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>17999</td>\n",
       "      <td>i feel like i just wanna buy any cute make up ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  label\n",
       "0               0                            i didnt feel humiliated      3\n",
       "1               1  i can go from feeling so hopeless to so damned...      3\n",
       "2               2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3               3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4               4                               i am feeling grouchy      3\n",
       "...           ...                                                ...    ...\n",
       "17995       17995  im having ssa examination tomorrow in the morn...      3\n",
       "17996       17996  i constantly worry about their fight against n...      2\n",
       "17997       17997  i feel its important to share this info for th...      2\n",
       "17998       17998  i truly feel that if you are passionate enough...      2\n",
       "17999       17999  i feel like i just wanna buy any cute make up ...      2\n",
       "\n",
       "[18000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd1=pd.read_csv(\"df_enco.csv\")\n",
    "pd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bbbb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "fina=[]\n",
    "def conver(data):\n",
    "    for val in range(len(data)):\n",
    "        lm = WordNetLemmatizer()\n",
    "        stopword=set(stopwords.words(\"english\"))\n",
    "        wor=re.sub(\"[^A-Za-z]\",' ',str(data[val]))\n",
    "        wor=wor.lower() \n",
    "        wor=wor.split()\n",
    "        wor = [lm.lemmatize(word) for word in wor if word not in set(stopwords.words('english'))]    \n",
    "        tex=[]\n",
    "        for ijx in wor:\n",
    "            if ijx not in stopword:\n",
    "                tex.append(lm.lemmatize(ijx))\n",
    "        fina.append(\" \".join(tex))\n",
    "    print(fina)\n",
    "    print(len(fina))    \n",
    "    \n",
    "train_fina=[]\n",
    "def train_conver(data):\n",
    "    for val in range(len(data)):\n",
    "        lm = WordNetLemmatizer()\n",
    "        stopword=set(stopwords.words(\"english\"))\n",
    "        wor=re.sub(\"[^A-Za-z]\",' ',str(data[val]))\n",
    "        wor=wor.lower() \n",
    "        wor=wor.split()\n",
    "        wor = [lm.lemmatize(word) for word in wor if word not in set(stopwords.words('english'))]    \n",
    "        tex=[]\n",
    "        for ijx in wor:\n",
    "            if ijx not in stopword:\n",
    "                tex.append(lm.lemmatize(ijx))\n",
    "        train_fina.append(\" \".join(tex))\n",
    "    print(train_fina)\n",
    "    print(len(train_fina))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20fc40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver(df.text)\n",
    "# train_conver(df_train.text)\n",
    "\n",
    "# fina_data=pd.DataFrame(fina,columns=[\"process\"])\n",
    "# fina_data.to_csv(\"processed.csv\")\n",
    "\n",
    "# train_da=pd.DataFrame(train_fina,columns=[\"process\"])\n",
    "# train_da.to_csv(\"train_process.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bedaf595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    didnt feel humiliated\n",
       "1        go feeling hopeless damned hopeful around some...\n",
       "2                im grabbing minute post feel greedy wrong\n",
       "3        ever feeling nostalgic fireplace know still pr...\n",
       "4                                          feeling grouchy\n",
       "                               ...                        \n",
       "17995    im ssa examination tomorrow morning im quite w...\n",
       "17996    constantly worry fight nature push limit inner...\n",
       "17997           feel important share info experience thing\n",
       "17998    truly feel passionate enough something stay tr...\n",
       "17999    feel like wanna buy cute make see online even one\n",
       "Name: process, Length: 18000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_data=pd.read_csv(\"processed.csv\")\n",
    "process_data[\"process\"]\n",
    "train_process=pd.read_csv(\"train_process.csv\")\n",
    "# train_process=pd.read_csv(\"custom_dattt.csv\")\n",
    "train_process[\"process\"]\n",
    "process_data[\"process\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9bc7d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector=CountVectorizer()\n",
    "x=vector.fit_transform(process_data[\"process\"])\n",
    "test=vector.transform(train_process[\"process\"])\n",
    "x.toarray()\n",
    "# test.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e83fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "param={'n_estimators':[100,200,300],\n",
    "      'criterion':['gini','entropy'],\n",
    "#       'max_depth':[10,20,30,40,50,60,70,80,90,100],\n",
    "#        'max_features':['auto','log2','sqrt'],\n",
    "#        'min_sample_leaf':[3,5,7,9,11],\n",
    "#        'min_samples_split': [2,5,10]\n",
    "      }\n",
    "# ,13,15,17,19,21,23,25 min sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0613f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier()\n",
    "grid=GridSearchCV(model,param,verbose=5,cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "262133fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grid.fit(x,df.label)  \n",
    "# grid.fit(x,pd.label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "520d3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(grid, open(\"grid_model_with_2_hp_22.pkl\", \"wb\"))\n",
    "# \n",
    "# gridd = pickle.load(open(\"grid_model_with_2_hp_22.pkl\", \"rb\"))\n",
    "\n",
    "# pickle.dump(grid, open(\"grid_model_with_2_hp_22_v2.pkl\", \"wb\"))\n",
    "# \n",
    "gridd = pickle.load(open(\"grid_model_with_2_hp_22_v2.sav\", \"rb\"))\n",
    "\n",
    "# pickle.dump(grid, open(\"grid_model_with_2_hp_22_v2.sav\", \"wb\"))\n",
    "\n",
    "# pickle.dump(vector, open(\"vector.sav\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e04d085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             verbose=5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45828cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([ 45.64792011,  88.43347309, 138.28526597,  47.3294764 ,\n",
      "        96.37987373, 127.95948763]), 'std_fit_time': array([ 1.2120378 ,  4.56925528,  5.13171048,  2.72902376, 10.20586326,\n",
      "        6.63700181]), 'mean_score_time': array([0.18518181, 0.36460633, 0.56332965, 0.18577433, 0.38442297,\n",
      "       0.52349634]), 'std_score_time': array([0.01733312, 0.02334599, 0.03052058, 0.01316714, 0.04087396,\n",
      "       0.02008944]), 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'entropy', 'entropy',\n",
      "                   'entropy'],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[100, 200, 300, 100, 200, 300],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'criterion': 'gini', 'n_estimators': 100}, {'criterion': 'gini', 'n_estimators': 200}, {'criterion': 'gini', 'n_estimators': 300}, {'criterion': 'entropy', 'n_estimators': 100}, {'criterion': 'entropy', 'n_estimators': 200}, {'criterion': 'entropy', 'n_estimators': 300}], 'split0_test_score': array([0.94166667, 0.94111111, 0.94166667, 0.94611111, 0.94444444,\n",
      "       0.94666667]), 'split1_test_score': array([0.94555556, 0.95055556, 0.95111111, 0.94888889, 0.95      ,\n",
      "       0.94888889]), 'split2_test_score': array([0.93777778, 0.94      , 0.94      , 0.94277778, 0.94277778,\n",
      "       0.94277778]), 'split3_test_score': array([0.94333333, 0.94388889, 0.94166667, 0.94333333, 0.94722222,\n",
      "       0.94722222]), 'split4_test_score': array([0.93444444, 0.93055556, 0.93388889, 0.94055556, 0.935     ,\n",
      "       0.93666667]), 'split5_test_score': array([0.93833333, 0.93777778, 0.94      , 0.93444444, 0.93777778,\n",
      "       0.93833333]), 'split6_test_score': array([0.93055556, 0.93277778, 0.92888889, 0.93166667, 0.935     ,\n",
      "       0.93388889]), 'split7_test_score': array([0.95333333, 0.95666667, 0.95333333, 0.95111111, 0.95277778,\n",
      "       0.955     ]), 'split8_test_score': array([0.93444444, 0.94      , 0.93944444, 0.93944444, 0.93833333,\n",
      "       0.94277778]), 'split9_test_score': array([0.93111111, 0.92944444, 0.93055556, 0.93333333, 0.93      ,\n",
      "       0.93166667]), 'mean_test_score': array([0.93905556, 0.94027778, 0.94005556, 0.94116667, 0.94133333,\n",
      "       0.94238889]), 'std_test_score': array([0.00671763, 0.00813941, 0.0074637 , 0.00624821, 0.00694867,\n",
      "       0.00692575]), 'rank_test_score': array([6, 4, 5, 3, 2, 1])}\n",
      "RandomForestClassifier(criterion='entropy', n_estimators=300)\n",
      "5\n",
      "{'criterion': 'entropy', 'n_estimators': 300}\n",
      "0.9423888888888889\n"
     ]
    }
   ],
   "source": [
    "print(gridd.cv_results_)\n",
    "print(gridd.best_estimator_)\n",
    "print(gridd.best_index_)\n",
    "print(gridd.best_params_)\n",
    "print(gridd.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e3233e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predic_grid_random=gridd.predict(test)\n",
    "acc_grid_random=accuracy_score(df_train.label,predic_grid_random)\n",
    "acc_grid_random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2e0c53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2507    0    2]\n",
      " [   1 6665    0]\n",
      " [   5    0 6820]]\n",
      "True Positive :2507\n",
      "False Negative :2\n",
      "False Positive :6\n",
      "True Negative :13485\n",
      "[[ 2507     2]\n",
      " [    6 13485]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "confusion_matrix(y_true=df_train.label,y_pred=predic_grid_random)\n",
    "\n",
    "\n",
    "\n",
    "result=confusion_matrix(y_true=df_train.label,y_pred=predic_grid_random)\n",
    "print(result)\n",
    "True_Positive =result[0][0]\n",
    "False_Negative=result[0][1]+result[0][2]\n",
    "False_Positive=result[1][0]+result[2][0]\n",
    "True_Negative=result[1][1]+result[2][2]\n",
    "print(f\"True Positive :{True_Positive}\")\n",
    "print(f\"False Negative :{False_Negative}\")\n",
    "print(f\"False Positive :{False_Positive}\")\n",
    "print(f\"True Negative :{True_Negative}\")\n",
    "matrix=[[True_Positive,False_Negative],[False_Positive,True_Negative]]\n",
    "print(np.matrix(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54c9b185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnterHi All. I have purchased OP Nord 2 for upgrade from my two years old Realme X. But my experience with Nord 2 is very bad. first its display is very bad quality compare to smartphone at 30k range. Even my Realme x display looking good against Nord 2. Its major highlight camera also very poor quality nowhere near to flagship sensor as OP advertising. Daylight pics are pathetic. After system update App Clone function is not working not able to use my 2nd whats app account. Call Recording function is not working which is my main requirement that is working flawless in my realme x. I think OnePlus make fool of us. Everyone who is purchasing will regret afterwards. So beware of OP Nord 2.\n",
      "['hi purchased op nord upgrade two year old realme x experience nord bad first display bad quality compare smartphone k range even realme x display looking good nord major highlight camera also poor quality nowhere near flagship sensor op advertising daylight pic pathetic system update app clone function working able use nd whats app account call recording function working main requirement working flawless realme x think oneplus make fool u everyone purchasing regret afterwards beware op nord']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\n",
    "\n",
    "inp=input(\"Enter\")\n",
    "# gridd.predict(test)\n",
    "train_fina=[]\n",
    "# for val in range(len(df_train.text)):\n",
    "lm = WordNetLemmatizer()\n",
    "stopword=set(stopwords.words(\"english\"))\n",
    "wor=re.sub(\"[^A-Za-z]\",' ',str(inp))\n",
    "wor=wor.lower()    \n",
    "wor=wor.split()\n",
    "wor = [lm.lemmatize(word) for word in wor if word not in set(stopwords.words('english'))]\n",
    "tex=[]\n",
    "for ijx in wor:\n",
    "    if ijx not in stopword:\n",
    "        tex.append(lm.lemmatize(ijx))\n",
    "train_fina.append(\" \".join(tex))\n",
    "print(train_fina)\n",
    "print(len(train_fina))        \n",
    "test=vector.transform(train_fina)\n",
    "predic=gridd.predict(test)\n",
    "# predic[0]\n",
    "\n",
    "# if(predic[0] == 1):\n",
    "#     print(\"Surprice\")\n",
    "    \n",
    "# if(predic[0] == 2):\n",
    "#     print(\"Joy\")\n",
    "# if(predic[0] == 3):\n",
    "#     print(\"Love\")\n",
    "# if(predic[0] == 4):\n",
    "#     print(\"Sadness\")\n",
    "# if(predic[0] == 5):\n",
    "#     print(\"Anger\")\n",
    "# if(predic[0] == 6):\n",
    "#     print(\"Fear\")\n",
    "\n",
    "\n",
    "# nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06f78fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't why the youtube community discourage this phone but it's now a month I have been using this phone and is the best ever device in 2021. Please don't hesitate, don't keep any second thoughts just go for it. One plus is one plus and not just ant brand out there and thanks to Amazon for providing me huge discount offer I got this product some what near 20ishhh. And I am delighted about it..\n",
      "['youtube community discourage phone month using phone best ever device please hesitate keep second thought go one plus one plus ant brand thanks amazon providing huge discount offer got product near ishhh delighted']\n",
      "1\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def check(inpp):\n",
    "    train_fina=[]\n",
    "    # for val in range(len(df_train.text)):\n",
    "    lm = WordNetLemmatizer()\n",
    "    stopword=set(stopwords.words(\"english\"))\n",
    "    wor=re.sub(\"[^A-Za-z]\",' ',str(inpp))\n",
    "    wor=wor.lower()    \n",
    "    wor=wor.split()\n",
    "    wor = [lm.lemmatize(word) for word in wor if word not in set(stopwords.words('english'))]\n",
    "    tex=[]\n",
    "    for ijx in wor:\n",
    "        if ijx not in stopword:\n",
    "            tex.append(lm.lemmatize(ijx))\n",
    "    train_fina.append(\" \".join(tex))\n",
    "    print(train_fina)\n",
    "    print(len(train_fina))        \n",
    "    test=vector.transform(train_fina)\n",
    "    predic=gridd.predict(test)\n",
    "    predic[0]\n",
    "\n",
    "    if(predic[0] == 1):\n",
    "        print(\"Neutral\")\n",
    "\n",
    "    if(predic[0] == 2):\n",
    "        print(\"Positive\")\n",
    "    if(predic[0] == 3):\n",
    "        print(\"Negative\")\n",
    "    # if(predic[0] == 4):\n",
    "    #     print(\"Sadness\")\n",
    "    # if(predic[0] == 5):\n",
    "    #     print(\"Anger\")\n",
    "    # if(predic[0] == 6):\n",
    "    #     print(\"Fear\")\n",
    "\n",
    "    # elif(predic[0] == 0):\n",
    "    #     print(\"sadness, anger, fear\")\n",
    "\n",
    "inp=input()\n",
    "check(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e0052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
